{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9922586b-14d1-47f3-bb2d-ea4de2725b8a",
   "metadata": {},
   "source": [
    "# Setting Up the Spark Session and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76e5385-96e8-474d-8180-e826ee6b4625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Telematics Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from a CSV file\n",
    "data_path = r\"C:\\allVehicles.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca74003-487e-4953-bdfc-b30f04f32921",
   "metadata": {},
   "source": [
    "# Data Cleansing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e565d411-520d-4474-8e81-af9d6a560d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.dropDuplicates(['deviceID', 'timestamp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f7e48-a185-4407-81c5-edcb29dfec62",
   "metadata": {},
   "source": [
    "# Transforming accData\n",
    "Applying the formulae provided in the document to using the @udf decorator from PySpark to create a UDF that follows the provided formula\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db8ce21-3d37-4fb0-bca8-e96ca593c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "@udf(returnType=FloatType())\n",
    "def convert_acc(acc_sample):\n",
    "    if len(acc_sample) == 162:  # Verify the sample length is as expected\n",
    "        mx = int(acc_sample[0:4], 16)\n",
    "        my = int(acc_sample[4:8], 16)\n",
    "        mz = int(acc_sample[8:12], 16)\n",
    "\n",
    "        # Adjust the values based on two's complement\n",
    "        mx = mx - 256 if mx > 127 else mx\n",
    "        my = my - 256 if my > 127 else my\n",
    "        mz = mz - 256 if mz > 127 else mz\n",
    "\n",
    "        # Convert to G-force\n",
    "        mx = mx * 0.01536\n",
    "        my = my * 0.01536\n",
    "        mz = mz * 0.01536\n",
    "\n",
    "        return (mx, my, mz)\n",
    "    else:\n",
    "        return (None, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3a179f-62d2-4074-b3fb-8426ca0eafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the UDF to the DataFrame\n",
    "df = df.withColumn(\"convertedAccData\", convert_acc(col(\"accData\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c2c0d-8f7d-413d-9c0c-3ecbc061405c",
   "metadata": {},
   "source": [
    "*Note: I didn't run this part as I get a Py4JJavaError Error due to wrong version of winutils for Hadoop on my environment can't fix due to **time constraints***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c85a4d-d804-43ff-be30-1b1b9d954626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Results\n",
    "df.select(\"deviceID\", \"timestamp\", \"accData\", \"convertedAccData\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6814d2-a631-4067-a188-b70c2d898c70",
   "metadata": {},
   "source": [
    "# Save or Continue Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847ee14-1765-4ef2-aa0b-4dfe97c1df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a new CSV file\n",
    "output_path = r\"C:\\cleaned_telematics_data.csv\"\n",
    "df.write.csv(output_path, mode=\"overwrite\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34458416-7843-4434-a48b-fb361b524044",
   "metadata": {},
   "source": [
    "# close spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7a449-199b-4806-be6a-09d156fcd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
